from langchain_core.vectorstores import InMemoryVectorStore
from openai import OpenAI


class RAG:
    """
    A class for implementing Retrieval-Augmented Generation (RAG).
    """

    def __init__(
        self,
        vector_store: InMemoryVectorStore,
        instructions_path: str,
        llm_model="gpt-4.1-nano",
    ):
        """
        Initializes the RAG object.

        Args:
            vector_store (InMemoryVectorStore): The vector store to use for retrieval.
            instructions_path (str): The path to the instructions file.
            llm_model (str, optional): The language model to use for generation. Defaults to "gpt-4.1-nano".
        """
        self.client = OpenAI()
        self.instructions = self._load_instructions(instructions_path)
        self.vector_store = vector_store
        self.llm_model = llm_model
        self.response = None

    def _build_context(self, query: str) -> str:
        """
        Builds the context for the query by retrieving relevant documents from the vector store.

        Args:
            query (str): The query to retrieve context for.

        Returns:
            str: A string containing the context for the query.
        """
        results = self.vector_store.similarity_search(query)
        return "\n\n".join([res for res in results])

    def _load_instructions(self, instructions_path: str) -> str:
        """
        Loads the instructions from the given file path.

        Args:
            instructions_path (str): The path to the instructions file.

        Returns:
            str: The instructions loaded from the file.

        Raises:
            e: If there is an error reading the file.
        """
        instructions = None
        try:
            with open(instructions_path, "r", encoding="utf-8") as f:
                instructions = f.read()
        except Exception as e:
            raise e
        return instructions

    def get_response(self, query: str):
        """
        Retrieves a response for the given query using RAG.

        Args:
            query (str): The query to respond to.

        Returns:
            _type_: The response generated by the language model.
        """
        context = self._build_context(query)
        response = self.client.responses.create(
            model=self.llm_model,
            instructions=self.instructions,
            input=f"Contexto: {context}\n\nPergunta: {query}",
        )
        return response


if __name__ is "__main__":
    pass
